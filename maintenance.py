#!/usr/bin/env python3
"""
Script d·ªçn d·∫πp v√† ki·ªÉm tra tr·∫°ng th√°i h·ªá th·ªëng
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path

def check_system_status():
    """Ki·ªÉm tra tr·∫°ng th√°i t·ªïng th·ªÉ c·ªßa h·ªá th·ªëng"""
    print("üîç KI·ªÇM TRA TR·∫†NG TH√ÅI H·ªÜ TH·ªêNG")
    print("=" * 50)
    
    # Check crawlers
    crawler_status = {
        'improved_crawler.py': 'active',
        'generic_crawler.py': 'backup', 
        'demo_crawler.py': 'deprecated',
        'lichviet_crawler.py': 'inactive',
        'lichvn_crawler.py': 'inactive',
        'tuvi_crawler.py': 'inactive'
    }
    
    print("\nüì¶ Crawler Status:")
    for crawler, status in crawler_status.items():
        icon = "‚úÖ" if status == 'active' else "‚ö†Ô∏è" if status == 'backup' else "‚ùå"
        print(f"  {icon} {crawler}: {status}")
    
    # Check data directories
    data_dirs = ['data/daily', 'data/monthly', 'data/raw', 'data/processed', 'logs']
    print("\nüìÅ Data Directories:")
    for directory in data_dirs:
        if os.path.exists(directory):
            files = len(os.listdir(directory))
            print(f"  ‚úÖ {directory}: {files} files")
        else:
            print(f"  ‚ùå {directory}: Not found")
    
    # Check recent activity
    print("\nüìä Recent Activity:")
    try:
        log_file = 'logs/crawler.log'
        if os.path.exists(log_file):
            with open(log_file, 'r') as f:
                lines = f.readlines()
                recent_lines = lines[-10:] if len(lines) >= 10 else lines
                print(f"  üìù Last {len(recent_lines)} log entries:")
                for line in recent_lines:
                    if 'INFO' in line and ('th√†nh c√¥ng' in line or 'success' in line.lower()):
                        print(f"    ‚úÖ {line.strip()}")
                    elif 'ERROR' in line or 'FAILED' in line:
                        print(f"    ‚ùå {line.strip()}")
        else:
            print("  ‚ö†Ô∏è No log file found")
    except Exception as e:
        print(f"  ‚ùå Error reading logs: {e}")

def cleanup_old_data():
    """D·ªçn d·∫πp d·ªØ li·ªáu c≈©"""
    print("\nüßπ D·ªåN D·∫∏P D·ªÆ LI·ªÜU C≈®")
    print("=" * 30)
    
    # Directories to clean
    cleanup_dirs = [
        'data/raw',
        'data/normalized', 
        'data/temp'
    ]
    
    total_cleaned = 0
    
    for directory in cleanup_dirs:
        if os.path.exists(directory):
            files = os.listdir(directory)
            # Keep only recent files (last 7 days)
            cutoff_time = datetime.now().timestamp() - (7 * 24 * 60 * 60)
            
            cleaned_count = 0
            for file in files:
                file_path = os.path.join(directory, file)
                if os.path.isfile(file_path):
                    file_time = os.path.getmtime(file_path)
                    if file_time < cutoff_time:
                        try:
                            os.remove(file_path)
                            cleaned_count += 1
                        except Exception as e:
                            print(f"    ‚ùå Cannot remove {file}: {e}")
            
            total_cleaned += cleaned_count
            print(f"  üóëÔ∏è {directory}: Removed {cleaned_count} old files")
        else:
            print(f"  ‚ö†Ô∏è {directory}: Directory not found")
    
    print(f"\n‚úÖ Total cleaned: {total_cleaned} files")

def test_improved_crawler():
    """Test improved crawler"""
    print("\nüß™ TEST IMPROVED CRAWLER")
    print("=" * 30)
    
    try:
        import sys
        sys.path.append('.')
        from crawlers.improved_crawler import ImprovedCalendarCrawler
        
        crawler = ImprovedCalendarCrawler()
        
        # Test API discovery
        print("üîç Testing API discovery...")
        apis = crawler.discover_real_apis()
        print(f"  Found {len(apis)} working APIs")
        
        # Test month crawl
        print("üìÖ Testing month crawl...")
        data = crawler.crawl_month(2025, 7)
        print(f"  Crawled {len(data)} days of data")
        
        if data:
            sample = data[0]
            print(f"  Sample: {sample.solar_date} | {sample.lunar_date} | {sample.can_chi_day}")
            
        print("‚úÖ Improved crawler test passed")
        
    except Exception as e:
        print(f"‚ùå Improved crawler test failed: {e}")

def generate_status_report():
    """T·∫°o b√°o c√°o tr·∫°ng th√°i"""
    print("\nüìã T·∫†O B√ÅO C√ÅO TR·∫†NG TH√ÅI")
    print("=" * 30)
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'system_status': 'operational',
        'active_crawlers': ['improved_crawler'],
        'inactive_crawlers': ['lichviet', 'lichvn', 'tuvi', 'demo'],
        'data_quality': 'hybrid_generated',
        'recommendations': [
            'T√¨m ki·∫øm API ch√≠nh x√°c h∆°n',
            'C·∫£i thi·ªán thu·∫≠t to√°n lunar calendar',
            'Th√™m validation cho d·ªØ li·ªáu can chi'
        ]
    }
    
    report_file = f"system_status_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"üìÑ B√°o c√°o ƒë√£ l∆∞u: {report_file}")

def main():
    """Main function"""
    print("üöÄ LICH CRAWLER - SYSTEM MAINTENANCE")
    print("=" * 50)
    print(f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # C√°c b∆∞·ªõc ki·ªÉm tra v√† d·ªçn d·∫πp
    check_system_status()
    cleanup_old_data()
    test_improved_crawler()
    generate_status_report()
    
    print("\nüéâ HO√ÄN TH√ÄNH!")
    print("=" * 20)
    print("‚úÖ H·ªá th·ªëng ƒë√£ ƒë∆∞·ª£c ki·ªÉm tra v√† d·ªçn d·∫πp")
    print("üìä Improved crawler ho·∫°t ƒë·ªông t·ªët")
    print("üîÑ S·∫µn s√†ng cho vi·ªác crawl d·ªØ li·ªáu")

if __name__ == "__main__":
    main()
