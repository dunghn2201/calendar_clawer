"""
Main entry point cho Lich Crawler
Cháº¡y file nÃ y Ä‘á»ƒ báº¯t Ä‘áº§u sá»­ dá»¥ng
"""

import sys
import os
from pathlib import Path

# ThÃªm project root vÃ o Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


def show_data_structure():
    """Hiá»ƒn thá»‹ cáº¥u trÃºc dá»¯ liá»‡u hiá»‡n táº¡i"""
    from data_manager import DataManager
    
    print("\nğŸ“Š Cáº¤U TRÃšC Dá»® LIá»†U HIá»†N Táº I")
    print("=" * 50)
    
    dm = DataManager()
    summary = dm.get_data_summary()
    
    print(f"ğŸ“ Tá»•ng sá»‘ nguá»“n: {summary['total_sources']}")
    print(f"ğŸ“„ Tá»•ng sá»‘ files: {summary['total_files']}")
    
    print("\nğŸ“‹ Chi tiáº¿t theo nguá»“n:")
    for source, details in summary["sources_detail"].items():
        status = "âœ…" if details['file_count'] > 0 else "â­•"
        print(f"  {status} {source:15} | {details['file_count']:2} files | {details['latest_records']:3} records")
    
    print("\nğŸ“ Cáº¥u trÃºc thÆ° má»¥c:")
    print("  data/")
    print("  â”œâ”€â”€ sources/              # Dá»¯ liá»‡u thÃ´ tá»« tá»«ng website")
    for source in summary["sources_detail"].keys():
        icon = "â”œâ”€â”€" if source != list(summary["sources_detail"].keys())[-1] else "â””â”€â”€"
        print(f"  â”‚   {icon} {source}/")
    print("  â”œâ”€â”€ processed/           # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½")
    print("  â”œâ”€â”€ merged/             # Dá»¯ liá»‡u Ä‘Ã£ ghÃ©p")
    print("  â””â”€â”€ backup/             # Backup")

def cleanup_old_data():
    """Dá»n dáº¹p dá»¯ liá»‡u cÅ© khÃ´ng cÃ³ cáº¥u trÃºc"""
    from data_manager import DataManager
    import os
    from pathlib import Path
    
    print("\nğŸ§¹ Dá»ŒN Dáº¸P Dá»® LIá»†U CÅ¨")
    print("=" * 50)
    
    data_dir = Path("data")
    old_files = [
        "test_1_processed.csv",
        "test_1_processed.db"
    ]
    
    moved_count = 0
    for old_file in old_files:
        old_path = data_dir / old_file
        if old_path.exists():
            backup_path = data_dir / "backup" / old_file
            backup_path.parent.mkdir(exist_ok=True)
            old_path.rename(backup_path)
            moved_count += 1
            print(f"ğŸ“¦ ÄÃ£ di chuyá»ƒn {old_file} vÃ o backup/")
    
    if moved_count > 0:
        print(f"âœ… ÄÃ£ di chuyá»ƒn {moved_count} files vÃ o thÆ° má»¥c backup")
    else:
        print("âœ… KhÃ´ng cÃ³ files cÅ© cáº§n dá»n dáº¹p")

def main():
    """Main function"""
    print("ğŸ—“ï¸ LICH CRAWLER - Vietnamese Lunar Calendar Data Scraper")
    print("=" * 60)
    
    # Kiá»ƒm tra dependencies
    missing_deps = check_dependencies()
    if missing_deps:
        print("âŒ Thiáº¿u dependencies:")
        for dep in missing_deps:
            print(f"  - {dep}")
        print("\nğŸ”§ Äá»ƒ cÃ i Ä‘áº·t, cháº¡y:")
        print("pip install -r requirements.txt")
        print("playwright install chromium")
        return
    
    print("âœ… Táº¥t cáº£ dependencies Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    
    # Import sau khi check dependencies
    try:
        from crawlers.demo_crawler import DemoCrawler, VietnameseCalendarAPI
        from crawlers.lichviet_crawler import LichVietCrawler
        from crawlers.lichvn_crawler import LichVnCrawler
        from crawlers.tuvi_crawler import TuviCrawler
        from crawlers.lichvannien_crawler import LichVannienCrawler
        from crawlers.lichngaytot_crawler import LichNgayTotCrawler
        from crawlers.licham365_crawler import LichAm365Crawler
        from crawlers.lichvannien365_crawler import LichVanNien365Crawler
        from processors.data_processor import LichDataProcessor
    except ImportError as e:
        print(f"âŒ Lá»—i import: {e}")
        print("Vui lÃ²ng cÃ i Ä‘áº·t dependencies trÆ°á»›c")
        return
    
    # Menu chÃ­nh
    while True:
        print("\nğŸ“‹ MENU CHÃNH:")
        print("1. ğŸš€ Test crawl má»™t trang web")
        print("2. ğŸ“… Crawl ngÃ y hÃ´m nay tá»« táº¥t cáº£ trang")
        print("3. ğŸ—“ï¸ Crawl thÃ¡ng hiá»‡n táº¡i")
        print("4. ğŸ“Š Xá»­ lÃ½ dá»¯ liá»‡u Ä‘Ã£ crawl")
        print("5. â° Báº¯t Ä‘áº§u scheduler tá»± Ä‘á»™ng")
        print("6. ğŸ“ˆ Xem thá»‘ng kÃª")
        print("7. ğŸ“Š Xem cáº¥u trÃºc dá»¯ liá»‡u")
        print("8. ğŸ§¹ Dá»n dáº¹p dá»¯ liá»‡u cÅ©")
        print("9. â“ HÆ°á»›ng dáº«n sá»­ dá»¥ng")
        print("10. ğŸšª ThoÃ¡t")
        
        choice = input("\nNháº­p lá»±a chá»n (1-10): ").strip()
        
        if choice == "1":
            test_single_crawler()
        elif choice == "2":
            crawl_today()
        elif choice == "3":
            crawl_current_month()
        elif choice == "4":
            process_data()
        elif choice == "5":
            start_scheduler()
        elif choice == "6":
            show_statistics()
        elif choice == "7":
            show_data_structure()
        elif choice == "8":
            cleanup_old_data()
        elif choice == "9":
            show_help()
        elif choice == "10":
            print("ğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng Lich Crawler!")
            break
        else:
            print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡")

def check_dependencies():
    """Kiá»ƒm tra dependencies cáº§n thiáº¿t"""
    required_packages = [
        'requests', 'beautifulsoup4', 'lxml', 'pandas', 
        'playwright', 'schedule', 'aiohttp'
    ]
    
    missing = []
    for package in required_packages:
        try:
            if package == 'beautifulsoup4':
                import bs4
            elif package == 'lxml':
                import lxml
            else:
                __import__(package)
        except ImportError:
            missing.append(package)
    
    return missing

def test_single_crawler():
    """Test má»™t crawler"""
    print("\nğŸ§ª TEST CRAWLER")
    print("Chá»n trang web Ä‘á»ƒ test:")
    print("1. Demo Crawler (luÃ´n hoáº¡t Ä‘á»™ng)")
    print("2. API Demo Crawler")  
    print("3. lichviet.app")
    print("4. lichvn.net") 
    print("5. tuvi.vn")
    print("6. lichvannien.net")
    print("7. lichngaytot.com")
    print("8. licham365.vn")
    print("9. lichvannien365.com")
    
    choice = input("Nháº­p lá»±a chá»n (1-9): ").strip()
    
    try:
        if choice == "1":
            from crawlers.demo_crawler import DemoCrawler
            crawler = DemoCrawler()
            print("ğŸš€ Testing Demo Crawler...")
        elif choice == "2":
            from crawlers.demo_crawler import VietnameseCalendarAPI
            crawler = VietnameseCalendarAPI()
            print("ğŸš€ Testing API Demo Crawler...")
        elif choice == "3":
            from crawlers.lichviet_crawler import LichVietCrawler
            crawler = LichVietCrawler()
            print("ğŸš€ Testing lichviet.app...")
        elif choice == "4":
            from crawlers.lichvn_crawler import LichVnCrawler
            crawler = LichVnCrawler()
            print("ğŸš€ Testing lichvn.net...")
        elif choice == "5":
            from crawlers.tuvi_crawler import TuviCrawler
            crawler = TuviCrawler()
            print("ğŸš€ Testing tuvi.vn...")
        elif choice == "6":
            from crawlers.lichvannien_crawler import LichVannienCrawler
            crawler = LichVannienCrawler()
            print("ğŸš€ Testing lichvannien.net...")
        elif choice == "7":
            from crawlers.lichngaytot_crawler import LichNgayTotCrawler
            crawler = LichNgayTotCrawler()
            print("ğŸš€ Testing lichngaytot.com...")
        elif choice == "8":
            from crawlers.licham365_crawler import LichAm365Crawler
            crawler = LichAm365Crawler()
            print("ğŸš€ Testing licham365.vn...")
        elif choice == "9":
            from crawlers.lichvannien365_crawler import LichVanNien365Crawler
            crawler = LichVanNien365Crawler()
            print("ğŸš€ Testing lichvannien365.com...")
        else:
            print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡")
            return
        
        # Test crawl thÃ¡ng 7/2024 (thay vÃ¬ 1/2024)
        data = crawler.crawl_month(2024, 7)
        print(f"âœ… Crawl thÃ nh cÃ´ng: {len(data)} ngÃ y")
        
        if data:
            print("\nğŸ“‹ Dá»¯ liá»‡u máº«u:")
            for i, item in enumerate(data[:5]):
                print(f"  {i+1}. {item.solar_date} - {item.lunar_date} - {item.can_chi_day}")
            
            # LÆ°u test data
            crawler.data = data
            filename = f"test_{choice}.json"
            crawler.save_to_json(filename)
            print(f"ğŸ’¾ ÄÃ£ lÆ°u vÃ o {filename}")
    
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

def crawl_today():
    """Crawl ngÃ y hÃ´m nay"""
    print("\nğŸ“… CRAWL NGÃ€Y HÃ”M NAY")
    
    try:
        from scheduler.auto_crawler import AutoCrawler
        crawler = AutoCrawler()
        results = crawler.crawl_today()
        
        print("\nğŸ“‹ Káº¿t quáº£:")
        for site, result in results.items():
            print(f"  {site}: {result}")
    
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

def crawl_current_month():
    """Crawl thÃ¡ng hiá»‡n táº¡i"""
    print("\nğŸ—“ï¸ CRAWL THÃNG HIá»†N Táº I")
    
    try:
        from scheduler.auto_crawler import AutoCrawler
        crawler = AutoCrawler()
        results = crawler.crawl_current_month()
        
        print("\nğŸ“‹ Káº¿t quáº£:")
        for site, result in results.items():
            print(f"  {site}: {result}")
    
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

def process_data():
    """Xá»­ lÃ½ dá»¯ liá»‡u"""
    print("\nğŸ“Š Xá»¬ LÃ Dá»® LIá»†U")
    
    # TÃ¬m file dá»¯ liá»‡u
    data_files = list(Path("data").rglob("*.json"))
    
    if not data_files:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u nÃ o")
        return
    
    print("Chá»n file Ä‘á»ƒ xá»­ lÃ½:")
    for i, file in enumerate(data_files):
        print(f"  {i+1}. {file}")
    
    try:
        choice = int(input("Nháº­p sá»‘: ")) - 1
        if 0 <= choice < len(data_files):
            from processors.data_processor import LichDataProcessor
            
            processor = LichDataProcessor(str(data_files[choice]))
            cleaned = processor.clean_and_process()
            
            if not cleaned.empty:
                stats = processor.get_statistics()
                print(f"âœ… Xá»­ lÃ½ thÃ nh cÃ´ng: {stats['total_records']} records")
                
                # Export
                base_name = str(data_files[choice]).replace('.json', '_processed')
                processor.export_to_sqlite(f"{base_name}.db")
                processor.export_to_csv(f"{base_name}.csv")
                print(f"ğŸ’¾ ÄÃ£ export: {base_name}.db, {base_name}.csv")
        else:
            print("âŒ Lá»±a chá»n khÃ´ng há»£p lá»‡")
    
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

def start_scheduler():
    """Báº¯t Ä‘áº§u scheduler"""
    print("\nâ° Báº®T Äáº¦U SCHEDULER Tá»° Äá»˜NG")
    print("Scheduler sáº½ crawl theo lá»‹ch trÃ¬nh:")
    print("  - HÃ ng ngÃ y: 06:00")
    print("  - HÃ ng tuáº§n: Chá»§ nháº­t 08:00")
    print("  - HÃ ng thÃ¡ng: NgÃ y 1 lÃºc 07:00")
    print("\nNháº¥n Ctrl+C Ä‘á»ƒ dá»«ng")
    
    try:
        from scheduler.auto_crawler import AutoCrawler
        crawler = AutoCrawler()
        crawler.start_scheduler()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ÄÃ£ dá»«ng scheduler")
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

def show_statistics():
    """Hiá»ƒn thá»‹ thá»‘ng kÃª"""
    print("\nğŸ“ˆ THá»NG KÃŠ Dá»® LIá»†U")
    
    # Äáº¿m files
    json_files = list(Path("data").rglob("*.json"))
    db_files = list(Path("data").rglob("*.db"))
    csv_files = list(Path("data").rglob("*.csv"))
    
    print(f"ğŸ“ Tá»•ng file dá»¯ liá»‡u: {len(json_files + db_files + csv_files)}")
    print(f"  - JSON: {len(json_files)}")
    print(f"  - SQLite: {len(db_files)}")
    print(f"  - CSV: {len(csv_files)}")
    
    # Thá»‘ng kÃª tá»« file gáº§n nháº¥t
    if json_files:
        latest_file = max(json_files, key=lambda f: f.stat().st_mtime)
        print(f"\nğŸ“„ File gáº§n nháº¥t: {latest_file.name}")
        
        try:
            import json
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"ğŸ“Š Sá»‘ records: {len(data)}")
            
            # Thá»‘ng kÃª nguá»“n
            sources = {}
            for item in data:
                source = item.get('source', 'unknown')
                sources[source] = sources.get(source, 0) + 1
            
            print("ğŸ“¡ Theo nguá»“n:")
            for source, count in sources.items():
                print(f"  - {source}: {count}")
        
        except Exception as e:
            print(f"âŒ Lá»—i Ä‘á»c file: {e}")

def show_help():
    """Hiá»ƒn thá»‹ hÆ°á»›ng dáº«n"""
    print("\nâ“ HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG")
    print("=" * 50)
    print("""
ğŸ¯ GIá»šI THIá»†U:
Lich Crawler lÃ  cÃ´ng cá»¥ Python Ä‘á»ƒ crawl dá»¯ liá»‡u lá»‹ch Ã¢m tá»« cÃ¡c trang web Viá»‡t Nam.

ğŸš€ BÆ¯á»šC Äáº¦U:
1. CÃ i Ä‘áº·t dependencies: pip install -r requirements.txt
2. CÃ i Ä‘áº·t browser: playwright install chromium
3. Cháº¡y: python main.py

ğŸ“Š CÃC CHá»¨C NÄ‚NG:
- Test crawler: Thá»­ nghiá»‡m crawl tá»« 1 trang web
- Crawl hÃ ng ngÃ y: Láº¥y dá»¯ liá»‡u ngÃ y hiá»‡n táº¡i
- Crawl thÃ¡ng: Láº¥y dá»¯ liá»‡u cáº£ thÃ¡ng
- Xá»­ lÃ½ dá»¯ liá»‡u: LÃ m sáº¡ch vÃ  export
- Scheduler: Tá»± Ä‘á»™ng crawl theo lá»‹ch trÃ¬nh

ğŸ’¾ Dá»® LIá»†U:
- LÆ°u trong thÆ° má»¥c data/
- Format: JSON, CSV, SQLite
- Tá»± Ä‘á»™ng backup vÃ  lÃ m sáº¡ch

ğŸ› ï¸ TROUBLESHOOTING:
- Lá»—i import: CÃ i Ä‘áº·t láº¡i dependencies
- Lá»—i network: Kiá»ƒm tra internet
- Lá»—i website: Thá»­ crawler khÃ¡c

ğŸ“ Há»– TRá»¢:
- Äá»c README.md Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t
- Check logs/ folder Ä‘á»ƒ debug
- GitHub Issues Ä‘á»ƒ bÃ¡o lá»—i
""")

if __name__ == "__main__":
    main()
